{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Activation functions\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def d_tanh(x):\n",
    "    return 1-np.square(np.tanh(x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def d_sigmoid(x):\n",
    "    return (1 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def leakyReLU(x, alpha=0.01):\n",
    "    return np.maximum(alpha*x,x)\n",
    "def d_leakyReLU(x, alpha=0.01):\n",
    "    temp = np.sign(x)\n",
    "    return np.maximum(alpha*temp,temp)\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "def d_ReLU(x):\n",
    "    temp = np.sign(x)\n",
    "    return np.maximum(0,temp)\n",
    "\n",
    "#Loss function \"c\"\n",
    "def logloss(y,a):\n",
    "    return -(y*np.log(a) + (1-y)*np.log(1-a))\n",
    "def d_logloss(y,a):\n",
    "    return (a - y)/(a*(1 - a))\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    activationFunctions = {\n",
    "        'tanh': (tanh, d_tanh),\n",
    "        'sigmoid': (sigmoid, d_sigmoid),\n",
    "        'ReLU': (ReLU, d_ReLU),\n",
    "        'leakyReLU': (leakyReLU, d_leakyReLU)\n",
    "    }\n",
    "\n",
    "    def __init__(self, inputs, neurons, activationF):                   #Number of inputs, neurons & type of activation function\n",
    "        self.w = np.random.randn(neurons, inputs) * np.sqrt(2./inputs)  #Initial weights\n",
    "        self.b = np.zeros((neurons, 1))                                 #Initial biases\n",
    "        self.act, self.d_act = self.activationFunctions.get(activationF)\n",
    "\n",
    "        self.m_dw = np.zeros((neurons, inputs))                         #First and second moments, mean and uncentered variance, weights\n",
    "        self.v_dw = self.m_dw\n",
    "        self.m_db = np.zeros((neurons, 1))                              #First and second moments, mean and uncentered variance, biases\n",
    "        self.v_db = self.m_db\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        self.x = x                                                      #Input from the previous layer\n",
    "        self.m = x.shape[1]                                             #Size of the batch\n",
    "        self.z = self.w @ self.x + self.b @ np.ones((1, self.m))        #Inputs times weights plus biases\n",
    "        self.y = self.act(self.z)                                       #Output from the current layer\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dJdy, learning_rate, lambd):\n",
    "        dJdz = np.multiply(dJdy, self.d_act(self.z))                    #dJdyl+1 * g'l = dJdz\n",
    "        dJdw = dJdz @ self.x.T                                          #dJdz * al-1 = dJdw\n",
    "        dJdb = dJdz @ np.ones((1, self.m)).T                            #dJdz * 1 = dJdb\n",
    "        dJdx = self.w.T @ dJdz                                          #Information for the next layer\n",
    "\n",
    "        reg = lambd/self.m*self.w                                       #Regularization term, only applied to the weights\n",
    "        dJdw += reg\n",
    "\n",
    "        self.w -= learning_rate * dJdw\n",
    "        self.b -= learning_rate * dJdb\n",
    "        return dJdx\n",
    "    \n",
    "    def Adam_backprop(self, dJdy, learning_rate, lambd, beta1, beta2, epsilon, epoch):\n",
    "        dJdz = np.multiply(dJdy, self.d_act(self.z))                    #dJdyl+1 * g'l = dJdz\n",
    "        dJdw = dJdz @ self.x.T                                          #dJdz * al-1 = dJdw\n",
    "        dJdb = dJdz @ np.ones((1, self.m)).T                            #dJdz * 1 = dJdb\n",
    "        dJdx = self.w.T @ dJdz                                          #Information for the next layer\n",
    "\n",
    "        reg = lambd/self.m*self.w                                       #Regularization term, only applied to the weights\n",
    "        dJdw += reg\n",
    "\n",
    "        self.m_dw = beta1*self.m_dw + (1-beta1)*dJdw                    #Mean\n",
    "        self.m_db = beta1*self.m_db + (1-beta1)*dJdb\n",
    "\n",
    "        self.v_dw = beta2*self.v_dw + (1-beta2)*np.power(dJdw, 2)       #Variance\n",
    "        self.v_db = beta2*self.v_db + (1-beta2)*np.power(dJdb, 2)\n",
    "\n",
    "        m_corr = 1-beta1**epoch                                         #Bias corrector terms\n",
    "        v_corr = 1-beta2**epoch\n",
    "\n",
    "        self.w -= learning_rate * np.divide(self.m_dw/m_corr, np.power(self.v_dw/v_corr,0.5)+epsilon)\n",
    "        self.b -= learning_rate * np.divide(self.m_db/m_corr, np.power(self.v_db/v_corr,0.5)+epsilon)\n",
    "        return dJdx\n",
    "\n",
    "class ANN:                                                              #X and Y must be in the matrix form (nÂºinputs or outputs,dataset size)\n",
    "\n",
    "    def __init__(self, nodes, activation):\n",
    "        self.nlayers = len(activation)\n",
    "        self.layers = [None]*self.nlayers\n",
    "        for i in range(self.nlayers):\n",
    "            self.layers[i] = Layer(nodes[i],nodes[i+1],activation[i])\n",
    "    def test(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.feedforward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, optimizer, lr, lambd):\n",
    "        costs = []\n",
    "        m = x_train.shape[1]\n",
    "\n",
    "        if optimizer == 'GD':                                           #Gradient descent\n",
    "            for epoch in range(epochs):\n",
    "                y = self.test(x_train)\n",
    "\n",
    "                dJdy = d_logloss(y_train, y)/m\n",
    "                for layer in reversed(self.layers):\n",
    "                    dJdy = layer.backprop(dJdy, lr, lambd)\n",
    "\n",
    "                J = 1/m *np.sum(logloss(y_train, y))\n",
    "                costs.append(J)\n",
    "\n",
    "        elif optimizer == 'SGD':                                        #Stochastic GD\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(y_train.shape[1]):\n",
    "                    y = self.test(x_train.T[i].reshape(-1,1))\n",
    "\n",
    "                    dJdy = d_logloss(y_train.T[i].reshape(-1,1), y)/1\n",
    "                    for layer in reversed(self.layers):\n",
    "                        dJdy = layer.backprop(dJdy, lr, lambd)\n",
    "                y = self.test(x_train)\n",
    "                J = 1/m *np.sum(logloss(y_train, y))\n",
    "                costs.append(J)\n",
    "\n",
    "        elif optimizer == 'Adam':                                       #Adam stochastic optimizer, recommended values: alfa = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-08\n",
    "            beta1 = 0.9\n",
    "            beta2 = 0.999\n",
    "            epsilon = 1e-08\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(y_train.shape[1]):\n",
    "                    y = self.test(x_train.T[i].reshape(-1,1))\n",
    "\n",
    "                    dJdy = d_logloss(y_train.T[i].reshape(-1,1), y)/1\n",
    "                    for layer in reversed(self.layers):\n",
    "                        dJdy = layer.Adam_backprop(dJdy, lr, lambd, beta1, beta2, epsilon, epoch+1)\n",
    "                \n",
    "                y = self.test(x_train)\n",
    "                J = 1/m *np.sum(logloss(y_train, y))\n",
    "                costs.append(J)\n",
    "        else:\n",
    "            print('No optimizer found. This library offers GD, SGD and Adam')\n",
    "\n",
    "        return costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## NN Training\n",
    "x_train = np.array([[0,0,1,1], [0,1,0,1]])\n",
    "y_train = np.array([[0,1,1,0]])\n",
    "#4           #Number of samples\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1000       #Times that the entire dataset is used to train the NN\n",
    "lr = 0.1            #Learning rate\n",
    "lambd = 0.001       #Regularization term\n",
    "nodes = [2, 5, 1]\n",
    "activation = ['ReLU','sigmoid']\n",
    "optimizer = 'GD'\n",
    "\n",
    "# NN and training\n",
    "neuralnetwork = ANN(nodes, activation)\n",
    "costs = neuralnetwork.train(x_train, y_train, epochs, optimizer, lr, lambd)\n",
    "\n",
    "plt.plot(range(epochs),costs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost function error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN Prediction\n",
    "x = x_train\n",
    "y = neuralnetwork.test(x)\n",
    "\n",
    "vect = np.rint(y)\n",
    "accuracy = np.sum(vect == y_train)/vect.shape[1]*100\n",
    "print('\\n Accuracy in %')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0], [0]])\n",
    "len(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "b126237f7ca490168c753438f1a9290c28cbd925e59b6904365c99f1dc93c750"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
