{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submarine features generator (+tectonic faults model +submarine height model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#Personal library for some machine learning alg. implementations\n",
    "import myML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "Input must be the heightmap with water colored black RGB(0,0,0), sea level at 128, land starts at 129."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tectonic faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load black-LAND (RGB 0,0,0) and white-SEA (RGB 255 255 255) image in grayscale mode\n",
    "imX = Image.open(r\"training/X.png\",\"r\").convert('L')\n",
    "imY1 = Image.open(r\"training/Y1.png\",\"r\").convert('L')\n",
    "display(imX.resize((1200,600), Image.Resampling.NEAREST))\n",
    "display(imY1.resize((1200,600), Image.Resampling.NEAREST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing image to land>=129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imData = np.array(imX)\n",
    "temp = np.min(imData[imData>0]), np.max(imData)\n",
    "temp = ((imData-temp[0])/(temp[1]-temp[0])*96+129).astype(np.uint8)\n",
    "for y0 in range(imData.shape[0]):\n",
    "    for x0 in range(imData.shape[1]):\n",
    "        if imData[y0,x0]>0:\n",
    "            imData[y0,x0] = temp[y0,x0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building first two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiusFinder(x0, imData, minPoints):\n",
    "    temp = []\n",
    "    lims = imData.shape\n",
    "    step = 5\n",
    "    for y0 in range(imData.shape[0]):\n",
    "        count = 0\n",
    "        for r in range(2,10000,step):\n",
    "            xlim = [max(0, x0-r), min(lims[1]-1, x0+r)]\n",
    "            ylim = [max(0, y0-r), min(lims[0]-1, y0+r)]\n",
    "\n",
    "            count = len(np.where(imData[ylim[0]:ylim[1]+1, xlim[0]:xlim[1]+1]>0)[0])\n",
    "            \n",
    "            if count>minPoints:\n",
    "                temp2 = r\n",
    "                break\n",
    "        if temp2==2: temp.append(temp2)\n",
    "        else:\n",
    "            for r in range(temp2+1-step,temp2+1):\n",
    "                xlim = [max(0, x0-r), min(lims[1]-1, x0+r)]\n",
    "                ylim = [max(0, y0-r), min(lims[0]-1, y0+r)]\n",
    "\n",
    "                count = len(np.where(imData[ylim[0]:ylim[1]+1, xlim[0]:xlim[1]+1]>0)[0])\n",
    "                \n",
    "                if count>minPoints: break\n",
    "            temp.append(r)\n",
    "            \n",
    "    return temp\n",
    "\n",
    "def inner_loop(x0, imData, imData2, convergent, divergent, adaptiveRadius, minPoints):\n",
    "    temp = []\n",
    "    lims = imData.shape\n",
    "    res = np.sqrt(lims[0]**2+lims[1]**2)\n",
    "    for y0 in range(imData.shape[0]):\n",
    "        radius = adaptiveRadius[y0,x0]\n",
    "        xlim = [max(0, x0-1-radius), min(lims[1]-1, x0+1+radius)]\n",
    "        ylim = [max(0, y0-1-radius), min(lims[0]-1, y0+1+radius)]\n",
    "        \n",
    "        temp2 = np.where(imData[ylim[0]:ylim[1]+1, xlim[0]:xlim[1]+1]>0)\n",
    "        landcount = len(temp2[0])\n",
    "        watercount = (1+ylim[1]-ylim[0])*(1+xlim[1]-xlim[0])-landcount\n",
    "\n",
    "        ranges = ylim[1]-ylim[0], xlim[1]-xlim[0]\n",
    "        center = (y0-ylim[0])/ranges[0], (x0-xlim[0])/ranges[1]\n",
    "        xlist, ylist = temp2[1]/ranges[1], temp2[0]/ranges[0]\n",
    "\n",
    "        distances = np.linalg.norm([xlist-center[1],ylist-center[0]], axis=0)\n",
    "        nearest = np.argsort(distances)[1:minPoints+1]\n",
    "        mindist = np.sqrt(((xlist[nearest[1]]-center[1])*ranges[1])**2+((ylist[nearest[1]]-center[0])*ranges[0])**2)/res\n",
    "        heights = [imData[ylim[0]+temp2[0][index],xlim[0]+temp2[1][index]] for index in nearest]\n",
    "        x_train = np.concatenate(([watercount/(watercount+landcount), mindist], (xlist-center[1])[nearest], (ylist-center[0])[nearest], np.array(heights)))\n",
    "\n",
    "        if imData2[y0,x0]==convergent: y_train = [1,0,0]\n",
    "        elif imData2[y0,x0]==divergent: y_train = [0,1,0]\n",
    "        else: y_train = [0,0,1]\n",
    "\n",
    "        temp.append([x_train, y_train])\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent = 0\n",
    "divergent = 255\n",
    "water = 0\n",
    "minPoints = 10\n",
    "\n",
    "imData = np.array(imX)\n",
    "imData2 = np.array(imY1)\n",
    "adaptiveRadius = np.zeros_like(imX)\n",
    "result = Parallel(n_jobs=-1)(delayed(radiusFinder)(x0, imData, minPoints) for x0 in tqdm(range(imData.shape[1])))\n",
    "\n",
    "for i, value in enumerate(result):\n",
    "    adaptiveRadius[:,i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Parallel(n_jobs=-1)(delayed(inner_loop)(x0, imData, imData2, convergent, divergent, adaptiveRadius, minPoints) for x0 in tqdm(range(imData.shape[1])))\n",
    "\n",
    "x_train, y_train = np.zeros((imData.shape[0]*imData.shape[1],minPoints*3+2)), np.zeros((imData.shape[0]*imData.shape[1],3))\n",
    "\n",
    "count = 0\n",
    "for x0 in result:\n",
    "    for elem in x0:\n",
    "        x_train[count,:], y_train[count,:] = elem[0], elem[1]\n",
    "        count+=1\n",
    "\n",
    "x_train = pd.DataFrame(x_train,columns=['Water percent','Mindist']+[f'x{i}' for i in range(minPoints)]+[f'y{i}' for i in range(minPoints)]+[f'h{i}' for i in range(minPoints)])\n",
    "y_train = pd.DataFrame(y_train,columns=['Convergent', 'Divergent', 'Normal'])\n",
    "pd.concat([x_train,y_train], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = max(len(x_train.index[y_train['Convergent']==1].to_list()), len(x_train.index[y_train['Convergent']==1].to_list()))\n",
    "temp = len(x_train.index[y_train['Normal']==1].to_list()) - temp\n",
    "temp = np.random.choice(x_train.index[y_train['Normal']==1].to_list(), size=temp, replace=False)\n",
    "x_train2 = x_train.loc[~x_train.index.isin(temp)].reset_index(drop=True)\n",
    "y_train2 = y_train.loc[~y_train.index.isin(temp)].reset_index(drop=True)\n",
    "\n",
    "temp = np.arange(len(x_train2))\n",
    "np.random.shuffle(temp)\n",
    "x_train2.iloc[:,:] = x_train2.iloc[temp,:]\n",
    "y_train2.iloc[:,:] = y_train2.iloc[temp,:]\n",
    "\n",
    "pd.concat([x_train2,y_train2], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400        #Times that the entire dataset is used to train the NN\n",
    "lr = 0.1            #Learning rate\n",
    "lambd = 0           #Regularization term\n",
    "activation = ['ReLU','ReLU','softmax']\n",
    "lossname = 'logloss'\n",
    "optimizer = 'Adam'\n",
    "\n",
    "generations=100\n",
    "popsize=10\n",
    "\n",
    "ni = 1\n",
    "nf = 20\n",
    "step = 2\n",
    "yplot_train = [[] for _ in range(2)]\n",
    "for i in tqdm(range(0, nf-ni+1, step)):\n",
    "    nodes = [minPoints*3+2, i+ni, i+ni, 3]\n",
    "\n",
    "    neuralnetwork = myML.ANN(nodes, activation, lossname)\n",
    "    neuralnetwork.defineNorm('Input', maximum=x_train2.max(), minimum=x_train2.min())\n",
    "    \n",
    "    _ = neuralnetwork.trainGA(x_train2, y_train2, generations=generations, popsize=popsize, tqdmDisable=True)\n",
    "    costs = neuralnetwork.train(x_train2, y_train2, epochs, optimizer, lr, lambd, tqdmDisable=True, batchSize=0, trainTestSplit=0.9)\n",
    "    yplot_train[0].append(costs[0][-1]), yplot_train[1].append(costs[1][-1])\n",
    "    #yplot_train[0].append(accuracy_score(y_train2, np.round(neuralnetwork.run(x_train2))))\n",
    "\n",
    "plt.plot(range(ni, nf-ni+1, step),yplot_train[0],label='Training set error')\n",
    "plt.plot(range(ni, nf-ni+1, step),yplot_train[1],label='Test set error')\n",
    "plt.xlabel('Nº of neurons in hidden layer')\n",
    "plt.ylabel('Cost function error')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500        #Times that the entire dataset is used to train the NN\n",
    "lr = 0.001          #Learning rate\n",
    "lambd = 0.5           #Regularization term\n",
    "activation = ['ReLU','ReLU','softmax']\n",
    "lossname = 'logloss'\n",
    "optimizer = 'Adam'\n",
    "nodes = [minPoints*3+2, 8, 8, 3]\n",
    "\n",
    "generations=100\n",
    "popsize=10\n",
    "\n",
    "neuralnetwork = myML.ANN(nodes, activation, lossname)\n",
    "neuralnetwork.defineNorm('Input', maximum=x_train2.max(), minimum=x_train2.min())\n",
    "\n",
    "gaCost = neuralnetwork.trainGA(x_train2, y_train2, generations=generations, popsize=popsize)\n",
    "costs = neuralnetwork.train(x_train2, y_train2, epochs, optimizer, lr, lambd, batchSize=0, trainTestSplit=0.8)\n",
    "\n",
    "plt.plot(range(generations),gaCost, label='GA loss')\n",
    "plt.plot(range(generations,generations+epochs),costs[0], label='Training loss')\n",
    "plt.plot(range(generations,generations+epochs),costs[1], label='Test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('Cost function error')\n",
    "plt.show()\n",
    "accuracy_score(y_train2, np.round(neuralnetwork.run(x_train2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralnetwork.storeClass(name=\"training/faultmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralnetwork = myML.ANN()\n",
    "neuralnetwork.loadClass(name=\"training/faultmodel\")\n",
    "\n",
    "im = np.array(imX.copy())\n",
    "points = neuralnetwork.run(x_train)\n",
    "count = 0\n",
    "for x in range(im.shape[1]):\n",
    "    for y in range(im.shape[0]):\n",
    "        temp = np.argmax(points[count,:])\n",
    "        if temp==0: #Convergent\n",
    "            im[y,x] = 0\n",
    "        elif temp==1: #Divergent\n",
    "            im[y,x] = 255\n",
    "        else: im[y,x] = 128\n",
    "        count+=1\n",
    "\n",
    "im = Image.fromarray(im)\n",
    "im.save('output files/step1.png')\n",
    "display(im.resize((1200,600), Image.Resampling.NEAREST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imY2 = Image.open(r\"training/Y2.png\",\"r\").convert('L')\n",
    "display(imY2.resize((1200,600), Image.Resampling.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_loop(x0, imData, imData2, faults, water, radius):\n",
    "    temp = []\n",
    "    lims = imData.shape\n",
    "    for y0 in range(imData.shape[0]):\n",
    "        if imData[y0,x0]!=water: \n",
    "            temp.append([False])\n",
    "            continue\n",
    "\n",
    "        xlim = [max(0, x0-1-radius), min(lims[1]-1, x0+1+radius)]\n",
    "        ylim = [max(0, y0-1-radius), min(lims[0]-1, y0+1+radius)]\n",
    "\n",
    "        faulty = faults[ylim[0]:ylim[1]+1,xlim[0]:xlim[1]+1]\n",
    "        x_train = [np.mean(faulty), np.std(faulty)]\n",
    "\n",
    "        y_train = imData2[y0,x0]\n",
    "\n",
    "        temp.append([x_train, y_train])\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = 0\n",
    "\n",
    "imData2 = np.array(imY2)\n",
    "faults = np.array(im)/255\n",
    "radius = int(np.ceil(np.sqrt(minPoints)))\n",
    "\n",
    "result = Parallel(n_jobs=-1)(delayed(inner_loop)(x0, imData, imData2, faults, water, radius) for x0 in tqdm(range(imData.shape[1])))\n",
    "\n",
    "x2_train, y2_train = np.zeros((imData.shape[0]*imData.shape[1],minPoints*3+4)), np.zeros((imData.shape[0]*imData.shape[1],1))\n",
    "count = 0\n",
    "count2 = 0\n",
    "for x0 in result:\n",
    "    for elem in x0:\n",
    "        if elem[0] is not False:\n",
    "            x2_train[count2,0:2], y2_train[count2,:] = elem[0], elem[1]\n",
    "            x2_train[count2,2:] = x_train.iloc[count,:]\n",
    "            count2+=1\n",
    "        count+=1\n",
    "\n",
    "x2_train = pd.DataFrame(x2_train,columns=['Fault mean', 'Fault std','Water percent','Mindist']+[f'x{i}' for i in range(minPoints)]+[f'y{i}' for i in range(minPoints)]+[f'h{i}' for i in range(minPoints)])\n",
    "y2_train = pd.DataFrame(y2_train,columns=['Elevation'])\n",
    "pd.concat([x2_train,y2_train], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = x2_train.index[x2_train['Water percent']==0].to_list()\n",
    "x2_train2 = x2_train.loc[~x2_train.index.isin(temp)].reset_index(drop=True)\n",
    "y2_train2 = y2_train.loc[~y2_train.index.isin(temp)].reset_index(drop=True)\n",
    "\n",
    "temp = np.arange(len(x2_train2))\n",
    "np.random.shuffle(temp)\n",
    "x2_train2.iloc[:,:] = x2_train2.iloc[temp,:]\n",
    "y2_train2.iloc[:,:] = y2_train2.iloc[temp,:]\n",
    "\n",
    "pd.concat([x2_train2,y2_train2], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation using the training samples to find the elbow of the curve (to avoid under or overfitting)\n",
    "epochs = 200        #Times that the entire dataset is used to train the NN\n",
    "lr = 0.001            #Learning rate\n",
    "lambd = 0           #Regularization term\n",
    "activation = ['leakyReLU','leakyReLU','ReLU']\n",
    "lossname = 'MSE'\n",
    "optimizer = 'Adam'\n",
    "\n",
    "generations=50\n",
    "popsize=5\n",
    "\n",
    "ni = 1\n",
    "nf = 15\n",
    "step = 2\n",
    "yplot_train = [[] for _ in range(2)]\n",
    "for i in tqdm(range(0, nf-ni+1, step)):\n",
    "    nodes = [minPoints*3+4, i+ni, i+ni, 1]\n",
    "\n",
    "    neuralnetwork = myML.ANN(nodes, activation, lossname)\n",
    "    neuralnetwork.defineNorm('Input', maximum=x2_train2.max(), minimum=x2_train2.min())\n",
    "    neuralnetwork.defineNorm('Output', maximum=y2_train2.max(), minimum=y2_train2.min())\n",
    "    \n",
    "    _ = neuralnetwork.trainGA(x2_train2, y2_train2, generations=generations, popsize=popsize, tqdmDisable=True)\n",
    "    costs = neuralnetwork.train(x2_train2, y2_train2, epochs, optimizer, lr, lambd, tqdmDisable=True, batchSize=0)\n",
    "    yplot_train[0].append(costs[0][-1]), yplot_train[1].append(costs[1][-1])\n",
    "\n",
    "plt.plot(range(0, nf-ni+1, step),yplot_train[0],label='Training set error')\n",
    "plt.plot(range(0, nf-ni+1, step),yplot_train[1],label='Test set error')\n",
    "plt.xlabel('Nº of neurons in hidden layer')\n",
    "plt.ylabel('Cost function error')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500        #Times that the entire dataset is used to train the NN\n",
    "lr = 0.001          #Learning rate\n",
    "lambd = 0.5           #Regularization term\n",
    "activation = ['ReLU','ReLU','ReLU']\n",
    "lossname = 'MSE'\n",
    "optimizer = 'Adam'\n",
    "nodes = [minPoints*3+4, 8, 8, 1]\n",
    "\n",
    "generations=100\n",
    "popsize=10\n",
    "\n",
    "neuralnetwork = myML.ANN(nodes, activation, lossname)\n",
    "neuralnetwork.defineNorm('Input', maximum=x2_train2.max(), minimum=x2_train2.min())\n",
    "neuralnetwork.defineNorm('Output', maximum=y2_train2.max(), minimum=y2_train2.min())\n",
    "\n",
    "gaCost = neuralnetwork.trainGA(x2_train2, y2_train2, generations=generations, popsize=popsize)\n",
    "costs = neuralnetwork.train(x2_train2, y2_train2, epochs, optimizer, lr, lambd, batchSize=0, trainTestSplit=0.8)\n",
    "\n",
    "plt.plot(range(generations),gaCost, label='GA loss')\n",
    "plt.plot(range(generations,generations+epochs),costs[0], label='Training loss')\n",
    "plt.plot(range(generations,generations+epochs),costs[1], label='Test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('Cost function error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralnetwork.storeClass(name=\"training/heightmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralnetwork = myML.ANN()\n",
    "neuralnetwork.loadClass(name=\"training/heightmodel\")\n",
    "\n",
    "fullHeightmap = np.array(imX.copy())\n",
    "points = neuralnetwork.run(x2_train)\n",
    "count = 0\n",
    "for x in range(fullHeightmap.shape[1]):\n",
    "    for y in range(fullHeightmap.shape[0]):\n",
    "        if imData[y,x]!=water: \n",
    "            fullHeightmap[y,x] = imData[y,x]\n",
    "            continue\n",
    "        fullHeightmap[y,x] = points[count,0]\n",
    "        count+=1\n",
    "\n",
    "# Application of gaussian blur with different radius to the diffused map + sum and normalization of the images\n",
    "blur_radius = [0,1]\n",
    "temp = sum([gaussian_filter(fullHeightmap, sigma=r)/len(blur_radius) for r in blur_radius])\n",
    "fullHeightmap = np.round(temp).astype(np.uint8)\n",
    "\n",
    "imDepth = Image.fromarray(fullHeightmap)\n",
    "imDepth.save('output files/step2.png')\n",
    "display(imDepth.resize((1200,600), Image.Resampling.NEAREST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakFinder(x0, imData):\n",
    "    temp = []\n",
    "    lims = imData.shape\n",
    "    for y0 in range(imData.shape[0]):\n",
    "        xlim = [max(0, x0-1), min(lims[1]-1, x0+1)]\n",
    "        ylim = [max(0, y0-1), min(lims[0]-1, y0+1)]\n",
    "\n",
    "        color = imData[y0,x0]\n",
    "        color = color-2 if color<127 else color*0.99\n",
    "\n",
    "        count = len(np.where(imData[ylim[0]:ylim[1]+1,xlim[0]:xlim[1]+1]>color)[0])\n",
    "        if count<=2: temp.append(255)\n",
    "        else: temp.append(0)\n",
    "\n",
    "    return temp\n",
    "\n",
    "peakData = np.empty_like(imData)\n",
    "result = Parallel(n_jobs=-1)(delayed(peakFinder)(x0, fullHeightmap) for x0 in tqdm(range(imData.shape[1])))\n",
    "\n",
    "for x0, res in enumerate(result):\n",
    "    for y0, value in enumerate(res):\n",
    "        peakData[y0,x0] = value\n",
    "\n",
    "Image.fromarray(peakData).save('output files/step3.png')\n",
    "display(Image.fromarray(peakData).resize((1200,600), Image.Resampling.NEAREST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erosion through Diffusion Limited Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakFinder(pos, fullHeightmap, simState, seed):\n",
    "    lims = simState.shape\n",
    "    random.seed(seed)\n",
    "\n",
    "    ydown, yup = [max(0, pos[0]-1), min(lims[0]-1, pos[0]+1)]\n",
    "    xdown, xup = [max(0, pos[1]-1), min(lims[1]-1, pos[1]+1)]\n",
    "        \n",
    "    #Check boundary for seed\n",
    "    if np.max(simState[ydown:yup+1,xdown:xup+1])>0: return [True, pos]\n",
    "\n",
    "    weights = [fullHeightmap[yup,x0]+1,fullHeightmap[y0,xup]+1,fullHeightmap[ydown,x0]+1,fullHeightmap[y0,xdown]+1]\n",
    "    paths = random.choices([0,1,2,3], weights=weights)[0]\n",
    "\n",
    "    if paths==0: pos[0] = yup\n",
    "    elif paths==1: pos[1] = xup\n",
    "    elif paths==2: pos[0] = ydown\n",
    "    else: pos[1] = xdown\n",
    "    return [False, pos]\n",
    "\n",
    "maxiter = 1000\n",
    "diffused = peakData.copy()\n",
    "\n",
    "temp = np.where(diffused==0)\n",
    "particles = int(np.sqrt(len(temp[0]))*100)\n",
    "threshold = int(np.sqrt(len(temp[0]))*4)\n",
    "points = np.random.randint(0, len(temp[0]), particles)\n",
    "points = [[temp[0][x],temp[1][x]] for x in points]\n",
    "seeds = random.sample(range(particles), particles)#np.random.randint(0, 32000, particles, dtype=int)\n",
    "\n",
    "pbar, nparticles = tqdm(range(maxiter)), particles\n",
    "for _ in pbar:\n",
    "    if nparticles<20000:\n",
    "        result = []\n",
    "        for i in range(nparticles):\n",
    "            result.append(peakFinder(points[i], fullHeightmap, diffused, seeds[i]))\n",
    "    else:\n",
    "        result = Parallel(n_jobs=-1)(delayed(peakFinder)(points[i], fullHeightmap, diffused, seeds[i]) for i in range(nparticles))\n",
    "\n",
    "    points = []\n",
    "    for i,elem in enumerate(result):\n",
    "        flag, pos = elem\n",
    "        if flag: diffused[pos[0],pos[1]] = 255*nparticles/particles\n",
    "        else: points.append(pos)\n",
    "\n",
    "    nparticles = len(points)\n",
    "    if nparticles<threshold:\n",
    "        temp = np.where(diffused==0)\n",
    "        if len(temp[0]>2000):\n",
    "            points.extend([[temp[0][x],temp[1][x]] for x in np.random.randint(0, len(temp[0]), 2000)])\n",
    "            nparticles+=2000\n",
    "    pbar.set_postfix_str(f'nparticles: {nparticles}')\n",
    "    if nparticles==0: break\n",
    "    \n",
    "random.seed()\n",
    "\n",
    "# Application of gaussian blur with different radius to the diffused map + sum and normalization of the images\n",
    "blur_radius = [2,8,16,32,64]\n",
    "temp = sum([gaussian_filter(diffused, sigma=r)/len(blur_radius) for r in blur_radius])\n",
    "diffused = np.round((temp-np.min(temp))*255/(np.max(temp)-np.min(temp))).astype(np.uint8)\n",
    "Image.fromarray(diffused).save('output files/step4.png')\n",
    "display(Image.fromarray(diffused).resize((1200,600), Image.Resampling.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, parameter):\n",
    "    return 1/(1+np.exp(-parameter*float(x-128)))\n",
    "\n",
    "parameter = 0.018\n",
    "erodedData = np.empty_like(imData)\n",
    "\n",
    "for x0 in range(imData.shape[1]):\n",
    "    for y0 in range(imData.shape[0]):\n",
    "        value = fullHeightmap[y0,x0]\n",
    "        erodedData[y0,x0] = value*func(temp[y0,x0],parameter)\n",
    "erodedData = np.round((erodedData-np.min(erodedData))/(np.max(erodedData)-np.min(erodedData))*255).astype(np.uint8)\n",
    "display(Image.fromarray(erodedData).resize((1200,600), Image.Resampling.NEAREST))\n",
    "Image.fromarray(erodedData).save('output files/step5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watercolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_loop(y0,imData,erodedData,waterlims,landlims,equator,temperate_center,artic_center,intermediate,m):\n",
    "    temp2 = []\n",
    "    equator_distance = abs(m*(y0/imData.shape[0]-equator)) #0 if at equator, 0.5 if at the poles \n",
    "    for x0 in range(imData.shape[1]):\n",
    "        height = erodedData[y0,x0]\n",
    "        \n",
    "        water = imData[y0,x0]\n",
    "        if water==0:\n",
    "            # Sea\n",
    "            height = height if height>waterlims[0] else waterlims[0]\n",
    "            if equator_distance<intermediate: \n",
    "                red = 0\n",
    "            else:\n",
    "                red = int(20*(equator_distance-intermediate)/(0.5-intermediate))\n",
    "\n",
    "            mingB = 0.73-0.17*equator_distance/0.5\n",
    "            max1gB = 1\n",
    "            max2gB = 1.24-0.24*equator_distance/0.5\n",
    "            minmod = 60-5*equator_distance/0.5\n",
    "            max1mod = 134-54*equator_distance/0.5\n",
    "            max2mod = 130-50*equator_distance/0.5\n",
    "\n",
    "            if height<=waterlims[1]:\n",
    "                heightmod = minmod+(max1mod-minmod)*(height-waterlims[0])/(waterlims[1]-waterlims[0])\n",
    "                gb = mingB+(max1gB-mingB)*(height-waterlims[0])/(waterlims[1]-waterlims[0])\n",
    "            else: \n",
    "                heightmod = max1mod+(max2mod-max1mod)*(height-waterlims[1])/(waterlims[2]-waterlims[1])\n",
    "                gb = max1gB+(max2gB-max1gB)*(height-waterlims[1])/(waterlims[2]-waterlims[1])\n",
    "            \n",
    "            blue = int(np.sqrt((heightmod**2-red**2)/(gb**2+1)))\n",
    "            green = int(gb*blue)\n",
    "        else:\n",
    "            # Land\n",
    "            height = height if height>landlims[0] else landlims[0]\n",
    "            height = height if height<landlims[1] else landlims[1]\n",
    "            if equator_distance<artic_center: \n",
    "                red = int(50*(artic_center-equator_distance)/artic_center)\n",
    "            else:\n",
    "                red = 0\n",
    "            if equator_distance<=temperate_center:\n",
    "                temp = equator_distance/temperate_center\n",
    "                blue = 0\n",
    "                maxmod = 67-4*temp\n",
    "                minmod = 64-16*temp\n",
    "            else:\n",
    "                temp = (equator_distance-temperate_center)/(0.5-temperate_center)\n",
    "                blue = int(85*temp)\n",
    "                maxmod = 63+54*temp\n",
    "                minmod = 48+53*temp\n",
    "\n",
    "            heightmod = minmod+(maxmod-minmod)*(height-landlims[0])/(landlims[1]-landlims[0])\n",
    "            green = int(np.sqrt(heightmod**2-blue**2-red**2))\n",
    "        temp2.append([red, green, blue])\n",
    "    return temp2\n",
    "\n",
    "equator = 0.5 #If 0 is the top, 1 the bottom, where is the equator in your map?\n",
    "temperate_center = 0.2 #If 0 is the equator, 0.5 the pole. You want this between Gibraltar and the Pirinees, or equivalent in your mod\n",
    "artic_center = 0.34 #If 0 is the equator, 0.5 the pole. You want this around Helsinki in Finland, or equivalent in your mod\n",
    "deviations = 1.5 #Deviations from the mean to make the ranges\n",
    "\n",
    "temp = np.mean(erodedData[imData==0]), np.std(erodedData[imData==0])\n",
    "waterlims = temp[0]-deviations*temp[1], temp[0]+deviations*temp[1], np.max(erodedData[imData==0])\n",
    "temp = np.mean(erodedData[imData>0]), np.std(erodedData[imData>0])\n",
    "landlims = temp[0]-deviations*temp[1], temp[0]+deviations*temp[1]\n",
    "\n",
    "m = -0.5/equator\n",
    "intermediate = (artic_center+temperate_center)/2\n",
    "watercolor = np.stack((erodedData.copy(),)*3, axis=-1)\n",
    "\n",
    "result = Parallel(n_jobs=-1)(delayed(inner_loop)(y0,imData,erodedData,waterlims,landlims,equator,temperate_center,artic_center,intermediate,m) for y0 in tqdm(range(imData.shape[0])))\n",
    "\n",
    "for y0, res in enumerate(result):\n",
    "    for x0, value in enumerate(res):\n",
    "        watercolor[y0,x0,:] = value\n",
    "    \n",
    "display(Image.fromarray(watercolor).resize((1200,600), Image.Resampling.NEAREST))\n",
    "Image.fromarray(watercolor).save('output files/watercolor_rgb_waterspec_a.png')\n",
    "print('Remember to convert it to dds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b22d928f5973cd7159b3034ea5d87b106ec4962028224788a5749e890f75777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
