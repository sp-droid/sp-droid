{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import myML\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    y = np.sin(x)+np.sin(2*x)+np.sin(5*x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halfwavenumber = 8\n",
    "wavesize = 20\n",
    "timestep = np.pi/2/wavesize\n",
    "t = np.array([np.linspace(0,np.pi*halfwavenumber,halfwavenumber*2*wavesize)])\n",
    "y = func(t)\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.figure(figsize=(24,6))\n",
    "ax = plt.axes()\n",
    "ax.plot(t[0,:],y[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralplot(nodes, condition, W, A):\n",
    "    g = nx.Graph()\n",
    "\n",
    "    L = len(nodes)\n",
    "    nmax = max(nodes)\n",
    "    nodesize = 3000/nmax\n",
    "    dic = {\n",
    "        'pos' : {},\n",
    "        'labels' : {},\n",
    "        'weights' : [],\n",
    "        'colors' : []\n",
    "    }\n",
    "    for l in range(L):\n",
    "        for j in range(nodes[l]):\n",
    "            nindex = (l+1)*100+j+1                                  #Neuron index\n",
    "            g.add_node(nindex)\n",
    "\n",
    "            y = ((nodes[l]-1)/2-j)                                  #Fixed position\n",
    "            if nmax>11: y =  y*nmax/nodes[l]\n",
    "            x = (l+1)\n",
    "            dic['pos'][nindex] = (x,y)\n",
    "            dic['labels'][nindex] = nindex-100\n",
    "\n",
    "            if l>0:                                                 #Synapses\n",
    "                for k in range(nodes[l-1]):\n",
    "                    nindexprev = l*100+k+1\n",
    "                    g.add_edge(nindexprev, nindex)\n",
    "\n",
    "    for j in range(nodes[0]):                                       #Input labels\n",
    "        nindex = 100+j+1\n",
    "        dic['labels'][nindex] = 'Input '+str(j+1)\n",
    "    for j in range(nodes[L-1]):                                     #Output labels\n",
    "        nindex = 100*L+j+1\n",
    "        dic['labels'][nindex] = 'Output '+str(j+1)\n",
    "\n",
    "    absmax = 1\n",
    "    for l in range(L):\n",
    "        absmax = max(absmax, np.max(np.absolute(A[l])))\n",
    "\n",
    "    if condition==True:\n",
    "        param = nodesize\n",
    "        nodesize = []\n",
    "        for l in range(1,L):\n",
    "            maxm = np.max(np.absolute(W[l]))\n",
    "            minm = np.min(np.absolute(W[l]))\n",
    "            for j in range(nodes[l]):\n",
    "                nindex = (l+1)*100+j+1\n",
    "                for k in range(nodes[l-1]):\n",
    "                    value = (abs(A[l][j][0]*W[l][j,k]/absmax)-minm)/(maxm-minm)\n",
    "                    dic['weights'].append(value*4+1)\n",
    "                    if A[l][j][0]*W[l][j,k]>0:\n",
    "                        dic['colors'].append((0, value*0.8+0.2, 0.2))\n",
    "                    elif A[l][j][0]*W[l][j,k]<0:\n",
    "                        dic['colors'].append((value*0.8+0.2, 0, 0.2))\n",
    "                    else:\n",
    "                        dic['colors'].append((0.1, 0.1, 0.1))\n",
    "        for l in range(L):\n",
    "            maxm = np.max(np.absolute(A[l]))\n",
    "            minm = np.min(np.absolute(A[l]))\n",
    "            for j in range(nodes[l]):\n",
    "                nindex = (l+1)*100+j+1\n",
    "                dic['labels'][nindex] = \"{:.3f}\".format(A[l][j][0])\n",
    "                value = (abs(A[l][j][0])-minm)/(maxm-minm)\n",
    "                if 1==nodes[l]: value=1\n",
    "                nodesize.append(value*0.7*param+0.5*param)\n",
    "                    \n",
    "\n",
    "    nx.draw(g, dic['pos'], labels=dic['labels'], with_labels=True, node_size=nodesize, font_size=3)\n",
    "    \n",
    "    if condition==True: nx.draw_networkx_edges(g,dic['pos'], width=dic['weights'], edge_color=dic['colors'])\n",
    "\n",
    "    aux = nx.Graph()                                                #Figure vertical size fix\n",
    "    aux.add_node('Aux1')\n",
    "    aux.add_node('Aux2')\n",
    "    posaux = {}\n",
    "    posaux['Aux1'], posaux['Aux2'] = [(0.9,nmax/2), (L+0.1,-nmax/2)]\n",
    "    nx.draw(aux, posaux, node_size=0)\n",
    "\n",
    "    plt.savefig('neuralplot/img{:05d}'.format(i), dpi=200)\n",
    "    plt.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def d_tanh(x):\n",
    "    return 1-np.square(np.tanh(x))\n",
    "def MSE(y,a):                                           #Mean squared error  --> Regression without outliers\n",
    "    m = y.shape[1]\n",
    "    return 2/m*(y-a)**2\n",
    "def d_MSE(y,a):\n",
    "    m = y.shape[1]\n",
    "    return -1/m*(y-a)\n",
    "def linear(x):                                          #Only used usually as output layer activation function\n",
    "    return x\n",
    "def d_linear(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def d_sigmoid(x):\n",
    "    return (1 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "def d_ReLU(x):\n",
    "    temp = np.sign(x)\n",
    "    return np.maximum(0,temp)\n",
    "\n",
    "class Layer:\n",
    "    activationFunctions = {\n",
    "        'tanh': (tanh, d_tanh),\n",
    "        'sigmoid': (sigmoid, d_sigmoid),\n",
    "        'ReLU': (ReLU, d_ReLU),\n",
    "        'linear': (linear, d_linear)\n",
    "    }\n",
    "\n",
    "    def __init__(self, inputs, neurons, activationF):                   #Number of inputs, neurons & type of activation function\n",
    "        self.w = np.random.randn(neurons, inputs) * np.sqrt(2./inputs)  #Initial weights\n",
    "        self.b = np.zeros((neurons, 1))                                 #Initial biases\n",
    "        self.act, self.d_act = self.activationFunctions.get(activationF)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        self.x = x                                                      #Input from the previous layer\n",
    "        self.m = x.shape[1]                                             #Size of the batch\n",
    "        self.z = self.w @ self.x + self.b @ np.ones((1, self.m))        #Inputs times weights plus biases\n",
    "        self.y = self.act(self.z)                                       #Output from the current layer\n",
    "        return self.y\n",
    "    \n",
    "    def backprop(self, dJdy, learning_rate, lambd):\n",
    "        dJdz = np.multiply(dJdy, self.d_act(self.z))                    #dJdyl+1 * g'l = dJdz\n",
    "        dJdw = dJdz @ self.x.T                                          #dJdz * al-1 = dJdw\n",
    "        dJdb = dJdz @ np.ones((1, self.m)).T                            #dJdz * 1 = dJdb\n",
    "        dJdx = self.w.T @ dJdz                                          #Information for the next layer\n",
    "\n",
    "        reg = lambd/self.m*self.w                                       #Regularization term, only applied to the weights\n",
    "        dJdw += reg\n",
    "\n",
    "        self.w -= learning_rate * dJdw\n",
    "        self.b -= learning_rate * dJdb\n",
    "            \n",
    "        return dJdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Forward Neural Network ([list with nodes per layer], [list with the name of the desired activation function in each non-input layer], [loss function name])\n",
    "#@test (X[d,m])\n",
    "#@testwithplot (X[d,m], numberoftestingsamples=1 (always, this is a failsafe), [list with nodes per layer])\n",
    "#@train (X[dx,m], Y[dy,m], nÂºepochs, type of optimizer, learning rate, regularization term)\n",
    "class simpleRNN:\n",
    "    lossfunctions = {\n",
    "        'MSE': (MSE, d_MSE)\n",
    "    }\n",
    "\n",
    "    def __init__(self, pastlength, activation, lossname):\n",
    "        self.pl = pastlength\n",
    "        nodes = [self.pl, self.pl, 1]\n",
    "        self.state = np.zeros((self.pl, 1))\n",
    "\n",
    "        self.nlayers = len(activation)\n",
    "        if self.nlayers != 2: print('Number of hidden layers must be one, 3 in total')\n",
    "        self.layers = [None]*self.nlayers                               #Layer creation\n",
    "        for i in range(self.nlayers):\n",
    "            self.layers[i] = Layer(nodes[i],nodes[i+1],activation[i])\n",
    "        self.loss, self.d_loss = self.lossfunctions.get(lossname)\n",
    "\n",
    "    def forwardprop(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.feedforward(x)\n",
    "        return x\n",
    "\n",
    "    def advanceintime(self):\n",
    "        y = self.forwardprop(self.state)\n",
    "        for elem in reversed(range(1,self.pl)): self.state[elem,0] = self.state[elem-1,0]\n",
    "        self.state[0,0] = y[0,0]\n",
    "        return y[0,0]\n",
    "\n",
    "    def advanceintimeplot(self):\n",
    "        l = 0\n",
    "        A = {}\n",
    "        temp = self.state*1\n",
    "        for layer in self.layers:\n",
    "            A[l] = temp\n",
    "            l += 1\n",
    "            temp = layer.feedforward(temp)\n",
    "        A[l] = temp\n",
    "\n",
    "\n",
    "        W = {}\n",
    "        for l in range(self.nlayers):\n",
    "            W[l+1] = self.layers[l].w\n",
    "        neuralplot([self.pl, self.pl, 1], True, W, A)\n",
    "\n",
    "        for elem in reversed(range(1,self.pl)): self.state[elem,0] = self.state[elem-1,0]\n",
    "        self.state[0,0] = temp[0,0]\n",
    "        return temp[0,0]\n",
    "\n",
    "    def resetstate(self, y_train):\n",
    "        k = 0\n",
    "        for i in reversed(range(y_train.shape[1])):\n",
    "            self.state[k,0] = y_train[0,i]\n",
    "            k+=1\n",
    "            if k==self.pl: break\n",
    "        return\n",
    "    \n",
    "    def backprop_learn(self, y_train, y, lr, lambd):\n",
    "        dJdy = self.d_loss(y_train, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            dJdy = layer.backprop(dJdy, lr, lambd)\n",
    "        return\n",
    "    \n",
    "    def train(self, t, y_train, lr):\n",
    "        costs = []\n",
    "        tt = [y_train[0,0]]\n",
    "        #In the first pass, we iterate through every timestep to obtain an acceptable initial condition\n",
    "        numsteps = t.shape[1]\n",
    "        self.state[0,0] = y_train[0,0]\n",
    "        for step in range(1, numsteps):\n",
    "            y = self.forwardprop(self.state)\n",
    "            self.backprop_learn(np.array([y_train[:,step]]), y, lr, lambd = 0)\n",
    "            J = np.sum(self.loss(np.array([y_train[:,step]]), y))\n",
    "            costs.append(J)\n",
    "            tt.append(y[0,0])\n",
    "            for elem in reversed(range(1,self.pl)): self.state[elem,0] = self.state[elem-1,0]\n",
    "            self.state[0,0] = y_train[0,step]\n",
    "\n",
    "        #Then we do the same but only update the weights at the end\n",
    "        self.resetstate(y_train[:,0:self.pl])\n",
    "        temp = np.zeros((self.pl, numsteps-self.pl))\n",
    "        for step in range(numsteps-self.pl):\n",
    "            temp[:,step] = self.state[:,0]\n",
    "            for elem in reversed(range(1,self.pl)): self.state[elem,0] = self.state[elem-1,0]\n",
    "            self.state[0,0] = y_train[0,step+self.pl]\n",
    "\n",
    "        for k in range(4000):\n",
    "            y = self.forwardprop(temp)\n",
    "            self.backprop_learn(y_train[:,self.pl-1:-1], y, lr, lambd = 0)\n",
    "            J = np.sum(self.loss(y_train[:,self.pl-1:-1],y))\n",
    "            if k % 100 == 0: print('Iter nÂº'+str(k)+', error: '+str(J))\n",
    "            costs.append(J)\n",
    "\n",
    "        #Ãdem, but now the state vector is updated with the network output\n",
    "        for k in range(4000):\n",
    "            self.resetstate(y_train[:,0:self.pl])\n",
    "            \n",
    "            for step in range(numsteps-self.pl):\n",
    "                temp2 = self.forwardprop(self.state)\n",
    "                temp[:,step] = self.state[:,0]\n",
    "                for elem in reversed(range(1,self.pl)): self.state[elem,0] = self.state[elem-1,0]\n",
    "                self.state[0,0] = temp2[0,0]\n",
    "        \n",
    "            y = self.forwardprop(temp)\n",
    "            self.backprop_learn(y_train[:,self.pl-1:-1], y, lr, lambd = 0)\n",
    "            J = np.sum(self.loss(y_train[:,self.pl-1:-1],y))\n",
    "            if k % 100 == 0: print('Iter nÂº'+str(k)+', error: '+str(J))\n",
    "            costs.append(J)           \n",
    "            \n",
    "        return costs, tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pastlength = 100\n",
    "nn = simpleRNN(pastlength, activation=['ReLU', 'sigmoid'], lossname='MSE')\n",
    "ynorm = myML.norm0to1_minmax(maxx=[4],minn=[-4]) #Bigger than the actual limits because we can't reach them with a sigmoid \n",
    "costs, tt = nn.train(t, ynorm.normalize(y), lr=0.1)\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yplot = ynorm.recover(np.array([tt]))[0,:]\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.figure(figsize=(24,6))\n",
    "ax = plt.axes()\n",
    "ax.plot(t[0,:],y[0,:], color='b')\n",
    "ax.plot(t[0,:],yplot, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.resetstate(ynorm.normalize(y))\n",
    "future = 320\n",
    "time = []\n",
    "yplot = np.zeros((1,future))\n",
    "for i in range(future):\n",
    "    yplot[0,i] = nn.advanceintime()\n",
    "    time.append(t[0,-1]+timestep*(i+1))\n",
    "yplot = ynorm.recover(yplot)[0,:]\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.figure(figsize=(24,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(time,yplot, color='g')\n",
    "\n",
    "ax.plot(t[0,:], y[0,:], color='b')\n",
    "yextra = func(np.array([time]))\n",
    "ax.plot(time,yextra[0,:], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.set_ylim([-1.2,1.2])\n",
    "legend_elements = [mpatches.Patch(color='b', label='sin(t)'),\n",
    "                   mpatches.Patch(color='g', label='prediction')]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "k=0\n",
    "for inst in range(t.shape[1]+future-1):\n",
    "    if inst % 50 == 0: print('Frame '+str(inst)+'out of '+str(t.shape[1]+future))\n",
    "\n",
    "    if inst < t.shape[1]:\n",
    "        ax.plot(t[0,inst:inst+2],y[0,inst:inst+2], color='b', linewidth=2)\n",
    "        ax.set_xlim([t[0,inst]-wavesize,t[0,inst]+wavesize])\n",
    "    elif inst == t.shape[1]:\n",
    "        ax.plot([t[0,-1], time[0]],[y[0,-1], yextra[0]], color='b', linewidth=2)\n",
    "        ax.set_xlim([t[0,-1]-wavesize,t[0,-1]+wavesize])\n",
    "    else:\n",
    "        ax.plot(time[k:k+2],yextra[k:k+2], color='b', linewidth=2)\n",
    "        ax.plot(time[k:k+2],yplot[k:k+2], color='g', linewidth=4)\n",
    "        k+=1\n",
    "        ax.set_xlim([time[k]-wavesize,time[k]+wavesize])\n",
    "    \n",
    "    plt.savefig('imgs/img{:05d}'.format(inst), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "b126237f7ca490168c753438f1a9290c28cbd925e59b6904365c99f1dc93c750"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
